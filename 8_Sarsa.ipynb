{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from utils.EnvironmentBasics import Environment, Action, State, ActionOrAid\n",
    "from utils.GridWorld import GridWorld\n",
    "from utils.AgentBasics import Agent, Policy\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingRewardState(State):\n",
    "    reward_i = 0\n",
    "    count = 0\n",
    "    interval_change_reward = 1000_000\n",
    "    reward_changed = False\n",
    "    def __init__(self, name: str, rewards: Union[np.ndarray, List[float]], is_terminal: bool = False, representation: Any = None) -> None:\n",
    "        super().__init__(name, rewards, is_terminal, representation)\n",
    "        self.n_rewards = len(rewards)\n",
    "\n",
    "    def getReward(self) -> float:\n",
    "        if self.is_terminal:\n",
    "            MovingRewardState.updateRewardi()\n",
    "        return self.rewards[self.__class__.reward_i % self.n_rewards]\n",
    "    \n",
    "    @classmethod\n",
    "    def updateRewardi(cls):\n",
    "        if cls.count % cls.interval_change_reward == 0:\n",
    "            cls.reward_i += 1\n",
    "            cls.reward_changed = True\n",
    "        cls.count += 1\n",
    "\n",
    "class GridWorldMovingReward(GridWorld):\n",
    "    def __init__(self, world_size: int, win_states: List[Tuple[int, int]] = None, fail_states: List[Tuple[int, int]] = None):\n",
    "        \"\"\" Rewrite GridWorld class, make state rewards more random\n",
    "\n",
    "        :param world_size: size of the world\n",
    "        \"\"\"\n",
    "        state_space = []\n",
    "        n_win_states = len(win_states)\n",
    "        i = 0\n",
    "        for row in range(world_size):\n",
    "            for col in range(world_size):\n",
    "                if (row, col) in win_states:\n",
    "                    is_terminal = True\n",
    "                    rewards = [0.2] * n_win_states\n",
    "                    rewards[i] = 1\n",
    "                    i += 1\n",
    "                    representation = \"W\"\n",
    "                elif (row, col) in fail_states:\n",
    "                    is_terminal = True\n",
    "                    rewards = [-2]\n",
    "                    representation = \"F\"\n",
    "                else:\n",
    "                    is_terminal = False\n",
    "                    rewards = [0]\n",
    "                    representation = \" \"\n",
    "                state_space.append(MovingRewardState(f\"({row},{col})\", rewards, is_terminal, representation=representation))\n",
    "\n",
    "        action_space = [Action(\"left\"), Action(\"right\"), Action(\"down\"), Action(\"up\")]\n",
    "        super().__init__(world_size, win_states + fail_states, state_space, action_space)\n",
    "        self.path = [self.current_sid]\n",
    "\n",
    "    def render(self, agent:Agent=None, figsize:Tuple[int, int]=None):\n",
    "        \"\"\" Render the environment, override method in Environment class\n",
    "        1. draw the states as grids\n",
    "        2. no need to draw transitions, because they are obvious in the grid world\n",
    "        3. highlight current state\n",
    "        4. highlight start state\n",
    "        5. highlight terminal states\n",
    "        \"\"\"\n",
    "        # if figsize is None:\n",
    "        #     figsize = (5, 5)\n",
    "        # plt.figure(figsize=figsize)\n",
    "\n",
    "        fig = Figure(figsize=figsize)\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        plt.xlim(-1, self.world_size)\n",
    "        plt.ylim(-1, self.world_size)\n",
    "        color = agent.state_value if agent is not None else [state.is_terminal for state in self.states]\n",
    "        ax.scatter(self.grid_colid, self.grid_rowid, c=color, marker=\"s\", s=1000, alpha=0.75, cmap=\"winter\", vmin=0, vmax=1)\n",
    "        path = [self.stateIdToCoord(sid) for sid in self.path]\n",
    "        ax.plot([p[0] for p in path], [p[1] for p in path], c=\"#000000\", linewidth=2)\n",
    "        for gi in range(self.n_grids):\n",
    "            label = f\"{agent.state_value[gi] if agent is not None else self.states[gi].is_terminal:.2f}\"\n",
    "            ax.annotate(label, (self.grid_colid[gi], self.grid_rowid[gi]), textcoords=\"offset points\", xytext=(2, -2), ha=\"center\")\n",
    "\n",
    "        self.highlightTerminalStates(ax)\n",
    "        if agent is not None:\n",
    "            for sid in range(self.n_states):\n",
    "                if not self.isTerminalState(sid):\n",
    "                    for action in self.getValidActionIds(sid):\n",
    "                        row, col = self.stateIdToCoord(sid)\n",
    "                        self.drawArrow(ax, col, row, action, agent.policy[sid, action])\n",
    "\n",
    "        canvas.draw()       # draw the canvas, cache the renderer\n",
    "        return np.frombuffer(canvas.tostring_rgb(), dtype='uint8').reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    def step(self, action: ActionOrAid) -> Tuple[Union[int, State], float, bool]:\n",
    "        \"\"\"\n",
    "        Step to the next state according to the current state and the action taken\n",
    "        :param action: action id or action\n",
    "        :return: next state id, reward, is_terminal\n",
    "        \"\"\"\n",
    "        assert self.isValidAction(self.current_sid, action), f\"Invalid action {action} in state {self.current_sid}\"\n",
    "        # According to the current state and the action taken, choose one of the next states based on the probabilities\n",
    "        next_sid = np.random.choice(self.n_states, p=self.transitions[self.current_sid, action, :])\n",
    "        next_state = self.sid_to_state[next_sid]\n",
    "        reward = next_state.getReward()\n",
    "        self.current_sid = next_sid\n",
    "        self.path.append(next_sid)\n",
    "        return next_sid, reward, next_state.is_terminal\n",
    "\n",
    "    def highlightTerminalStates(self, ax):\n",
    "        \"\"\" Highlight terminal states \"\"\"\n",
    "        for row in range(self.world_size):\n",
    "            for col in range(self.world_size):\n",
    "                if self.sid_to_state[self.coordToStateId(row, col)].representation == \"W\":\n",
    "                    ax.scatter(col, row, c=\"green\", marker=\"s\", s=700)\n",
    "                elif self.sid_to_state[self.coordToStateId(row, col)].representation == \"F\":\n",
    "                    ax.scatter(col, row, c=\"red\", marker=\"s\", s=700)\n",
    "\n",
    "    \n",
    "    def drawArrow(self, ax, x: int, y: int, action: Union[Action, int], length: float = 1):\n",
    "        \"\"\" Draw an arrow from point x, y, direction code: {0: \"left\", 1: \"right\", 2: \"down\", 3: \"up\"} \"\"\"\n",
    "        if not isinstance(action, Action):\n",
    "            action = self.aid_to_action[action]\n",
    "        dx, dy = 0, 0\n",
    "        if action.name == \"left\":\n",
    "            dx = np.clip(-1 * length, 0, 0.9)\n",
    "        elif action.name == \"right\":\n",
    "            dx = np.clip(1 * length, 0, 0.9)\n",
    "        elif action.name == \"down\":\n",
    "            dy = np.clip(-1 * length, 0, 0.9)\n",
    "        elif action.name == \"up\":\n",
    "            dy = np.clip(1 * length, 0, 0.9)\n",
    "\n",
    "        ax.arrow(x, y, dx, dy, head_width=0.1, head_length=0.1, length_includes_head=True)\n",
    "\n",
    "\n",
    "    def reset(self, start_state_id: int=-1) -> int:\n",
    "        \"\"\" Reset the environment to the start state\n",
    "        If start_state_id is not specified or set to -1, choose a random start state\n",
    "\n",
    "        :param start_state_id: the id of the start state, defaults to -1\n",
    "        :return: the start state id\n",
    "        \"\"\"\n",
    "        if start_state_id == -1:\n",
    "            self.start_sid = np.random.randint(self.n_states)\n",
    "        else:\n",
    "            self.start_sid = start_state_id\n",
    "        self.current_sid = self.start_sid\n",
    "        self.path = [self.current_sid]\n",
    "        return self.start_sid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa: On-Policy TD Control\n",
    "当谈到Control的时候，就是指我们在循环的过程中，action-value和policy不断地交替更新，最终实现收敛，也就是Generalized Policy Iteration (GPI)。<p>\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s',a') - Q(s,a) \\right]$$\n",
    "书里原公式为：\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \\right]$$\n",
    "这个公式就是TD(0)的公式换成了action-value的更新。<p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(S_t,A_t)$是当前状态和动作的action-value，我们要bootstrap，因此就是要用旧的来估计新的，注意等式右边除了$R_{t+1}$没一个东西是新的<p>\n",
    "$R_{t+1} + \\gamma Q(S_{t+1},A_{t+1})$是我们根据实际获得的reward和下一个状态的旧action-value算出来的新的action-value估计值。<p>\n",
    "那么$R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)$就是我们的error，是新估计值和旧估计值的差。<p>\n",
    "然后这个差乘上step-size，就是我们要更新的量。最后把这个更新的量加到当前action-value上，就更新了action-value<p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>注意！！！一定要在terminal state时设置terminal state的action-value因为在terminal state时你拿不到下一个action-value，因此我之前把它忽略掉了，导致action-value在碰到terminal state时无法得到来自terminal state的reward(其实是-1的penalty)，最终导致policy不知道去躲避陷阱terminal state。</mark><"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sarsa:  95%|█████████▌| 9500/10000 [01:24<00:04, 112.73it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Sarsa(grid_world: GridWorldMovingReward):\n",
    "    agent = Agent(grid_world, Policy(grid_world))\n",
    "    # Make state value of terminal state\n",
    "    for sid in grid_world.getTerminalStateIds():\n",
    "        agent.state_value[sid] = grid_world.states[sid].getReward()\n",
    "    gamma = 0.9\n",
    "    step_size = 0.2\n",
    "\n",
    "    for it in tqdm(range(10000), desc=\"Sarsa\"):\n",
    "        # Generate episode\n",
    "        is_terminal = True\n",
    "        while is_terminal:\n",
    "            grid_world.reset(0)  # reset with a random start state\n",
    "            is_terminal = grid_world.isTerminalState(grid_world.start_sid)\n",
    "\n",
    "        # iterate over all state action pairs to reset policy\n",
    "        agent.policy = agent.action_value.copy(grid_world)\n",
    "        agent.policy.normalize()\n",
    "\n",
    "        old_sid = grid_world.start_sid\n",
    "        old_aid = agent.takeAction(old_sid, epsilon=0.4)\n",
    "\n",
    "        while True:\n",
    "            # random_action_id = np.random.randint(0, grid_world.n_actions)\n",
    "            new_sid, r, is_terminal = grid_world.step(old_aid)\n",
    "\n",
    "            agent.state_value[old_sid] += step_size * (r + gamma * agent.state_value[new_sid] - agent.state_value[old_sid])\n",
    "\n",
    "            if is_terminal:\n",
    "                ### --- Action value for terminal state --- ###\n",
    "                agent.action_value[old_sid, old_aid] += step_size * (r - agent.action_value[old_sid, old_aid])\n",
    "                # agent.action_value[old_sid, old_aid] = 0\n",
    "                break\n",
    "            new_aid = agent.takeAction(new_sid, epsilon=0.4)\n",
    "\n",
    "            agent.action_value[old_sid, old_aid] += step_size * (r + gamma * agent.action_value[new_sid, new_aid] - agent.action_value[old_sid, old_aid])\n",
    "\n",
    "            # agent.policy[old_sid, old_aid] = agent.state_value[old_sid]\n",
    "            # # agent.policy[old_sid, old] = agent.action_value[old_sid]\n",
    "            # av_sum = np.sum(agent.action_value[old_sid])\n",
    "            # if av_sum != 0:\n",
    "            #     agent.policy[old_sid] /= av_sum\n",
    "            # else:\n",
    "            #     agent.policy[old_sid] = 0.25\n",
    "            # best_action_ids = agent.getBestActions(old_sid)\n",
    "            # agent.policy[old_sid] = 0\n",
    "            # agent.policy[old_sid, best_action_ids] = 1\n",
    "            # agent.policy[old_sid] /= np.sum(agent.policy[old_sid])\n",
    "\n",
    "            old_aid = new_aid\n",
    "            old_sid = new_sid\n",
    "\n",
    "        if MovingRewardState.reward_changed:\n",
    "            for sid in grid_world.getTerminalStateIds():\n",
    "                agent.state_value[sid] = grid_world.states[sid].getReward()\n",
    "            MovingRewardState.reward_changed = False\n",
    "\n",
    "        if it % 100 == 0:\n",
    "            agent.policy.normalize()\n",
    "            img = grid_world.render(agent, figsize=(10, 10))\n",
    "            cv2.imshow(\"img\", img)\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord(\"q\"):\n",
    "                break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "Sarsa(GridWorldMovingReward(world_size=16, \n",
    "                        win_states=[(10, 2), (4, 10), (14, 14)], \n",
    "                        fail_states=[(4, 4), (6, 6), (10, 10), (12, 12)]\n",
    "                        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
