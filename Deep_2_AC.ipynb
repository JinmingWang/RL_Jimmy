{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic (A2C) Method\n",
    "Actor: 在这里，Actor意为“行动者”，它是整体算法本身，如果我们只用Actor而没有Critic，那么这个算法就变成了普通的policy gradient方法。它根据state输出policy，要注意，不同于输出action value，这里输出的policy是一个概率分布，所有选择的和必须等于1，这里一般用softmax函数来实现。<p>\n",
    "Critic: 在这里意为“评审者”，它会尝试预测state value，一般用TD方法。它用于评估我们采取的action有多好，具体解释如下。<p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor\n",
    "用于根据state选择action，即policy，一般用policy gradient方法\n",
    "$$ \\Delta \\theta = \\alpha \\nabla_\\theta J(\\pi_\\theta) = \\alpha \\sum_{s,a} \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s) b(s,a) $$\n",
    "|符号|含义|\n",
    "|:--|:--|\n",
    "|$\\theta$|policy模型参数，也是Actor的参数|\n",
    "|$\\alpha$|学习率|\n",
    "|$J(\\pi_\\theta)$|policy的期望回报|\n",
    "|$\\nabla_\\theta J(\\pi_\\theta)$|梯度的期望形式|\n",
    "|$\\pi_\\theta(a\\|s)$|根据state选择action的概率，即根据当前模型参数下的policy|\n",
    "|$\\nabla_\\theta \\log \\pi_\\theta(a\\|s)$|这是policy的梯度，但是用log形式表示|\n",
    "|$b(s,a)$|Baseline function|\n",
    "|$\\nabla_\\theta \\log \\pi_\\theta(a\\|s) b(s,a)$|这是这个policy对应的state-action value的梯度，即你选了这个action对应的价值的梯度|\n",
    "|$\\pi_\\theta(a\\|s) \\nabla_\\theta \\log \\pi_\\theta(a\\|s) b(s,a)$|选择这个action的概率乘以这个action对应的价值的梯度，这里的policy概率起到了了一个类似权重的作用|\n",
    "\n",
    "此公式的意义是：对于所有可能的action，对他们的价值的梯度进行加权求和，这样就可以得到policy对于这个state的总更新量，然后根据学习率进行更新。\n",
    "|baseline function|含义|\n",
    "|:--|:--|\n",
    "|$b(s,a) = Q(s,a)$|这是最简单的baseline，就是state-action value|\n",
    "|$b(s,a) = Q(s,a) - V(s)$|state-action value减去state value|\n",
    "|$b(s,a) = A(s,a)$|advantage function|\n",
    "|$b(s,a) = \\delta$|TD error|\n",
    "\n",
    "使用Pytorch更新过程中，我们直接这样计算梯度:\n",
    "```python\n",
    "...\n",
    "batch_policy = policy_model(batch_state)\n",
    "neg_log_prob = -torch.log(batch_policy.gather(1, batch_action.unsqueeze(1)).squeeze(1))\n",
    "loss = (neg_log_prob * batch_advantage).mean()\n",
    "loss.backward()\n",
    "...\n",
    "```\n",
    "如上所示，直接用选取的action对应的policy概率乘以advantage function，然后求平均，这样就可以得到actor loss。<p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic\n",
    "用于评估state-action对的价值，即value function，一般用TD方法。由于Critic输出的是state value因此采用得是如下公式：\n",
    "$$ \\Delta \\theta = \\alpha (r + \\gamma V_\\theta(s') - V_\\theta(s))$$\n",
    "|符号|含义|\n",
    "|:--|:--|\n",
    "|$\\theta$|value function模型参数，也是Critic的参数|\n",
    "|$\\alpha$|学习率|\n",
    "|$r$|reward|\n",
    "|$\\gamma$|discount|\n",
    "|$V_\\theta(s)$|state value|\n",
    "|$r + \\gamma V_\\theta(s')$|是TD target，即采取了Action之后实际估算出来的state value|\n",
    "|$r + \\gamma V_\\theta(s') - V_\\theta(s)$|是TD error，就是采取了行动后估算出的return减去我们预估当前state的return|\n",
    "\n",
    "Critic的全部意义在于计算期望差，由于我们采取行动是根据当前的policy来的，因此$r+\\gamma V_\\theta(s')$是我们采取行动后实际的立即收益加估算的未来收益，即当前state采取特定action后的总收益，而$V_\\theta(s)$是我们旧的对当前state的总收益，两者之间的区别在于前者更实际，且受action影响。可能不巧正好采用了一个很烂的action，因此观测到后者比前者大。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage function\n",
    "这个函数是上述baseline function的特殊情况，即：\n",
    "|advantage function|含义|\n",
    "|:--|:--|\n",
    "|$Q(s,a) - V(s)$|state value $V(s)$是在state s的价值，某种程度上考虑了所有可能的action，相当于当前state上所有action的综合期待，这里简单地假设$V(s)$为所有$Q(s, ?)$的加权平均，而$Q(s,a)$是特定action的期待价值，那么他们俩的差就定义了在当前state s，这个特定action a的价值是否能比得上当前所有action的平均价值。打个比方，一个班级里特定学生成绩好坏不仅取决于分数，也取决于班级平均成绩，只看分数不看平均成绩就无法客观评估学生的情况，而用特定学生成绩减去平均成绩就是一个很好的衡量指标。|\n",
    "|$r + \\gamma V_\\theta(s') - V_\\theta(s)$|TD error。公式减号前的部分起到了上面公式类似$Q(s,a)$的效果，毕竟它也是一种采取了特定行动后获得的价值|\n",
    "|$r_{t+1} + + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... + \\gamma^{t+n} V_\\theta(s_{t+n}) - V_\\theta(s_t)$|n-step TD error|\n",
    "\n",
    "上面的所有公式都是advantage function，都可以用。<p>\n",
    "他们的共同点都是：用采取了行动之后真实获得的奖励，减去预估的、期待的或某种意义上平均的奖励，这样你就拥有一个比较，即特定action的价值是否比平均价值高，即特定action是否有**Advantage**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
