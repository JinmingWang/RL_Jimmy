{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.EnvironmentBasics import Environment, Action, State\n",
    "from utils.GridWorld import GridWorld\n",
    "from utils.AgentBasics import Agent, Policy\n",
    "from tqdm import tqdm\n",
    "from typing import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning\n",
    "Monte Carlo 方法等到episode结束才能更新，因为episode结束之前完全不知道G_t的值，其更新公式如下：\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right] $$\n",
    "Temporal-Difference 方法不必等到episode结束，它借用了当前agent对下一个state $V(S_{t+1})$ 的估计值和完成当前action后的reward $R_{t+1}$ 来更新当前state $V(S_t)$ 的估计值，就像Dynamic Programming一样，都是为了实况地更新当前state的value，只得使用其他state的旧value来对未来做一个估计，其更新公式如下：\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right] $$\n",
    "MC方法对未来收益的估计是$G_t$，而TD方法对未来收益的估计是$R_{t+1} + \\gamma V(S_{t+1})$，而别忘了$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots$，因此可以可以知道$\\gamma V(S_{t+1})$实际上是对$G_t$后半部分$\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots$的一个不准确的估计。在最好的情况下，$G_t$和$R_{t+1} + \\gamma V(S_{t+1})$是一样的，但是在最坏的情况下，$G_t$和$R_{t+1} + \\gamma V(S_{t+1})$是完全不同的，因此TD方法的估计值是不准确的，但是它的估计值是更实时的。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
